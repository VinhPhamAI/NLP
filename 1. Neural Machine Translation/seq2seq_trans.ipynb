{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tCHONZnMa8zy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "5khEpvNvbNrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03275e58-ad0c-4e56-befe-59f6369707f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data"
      ],
      "metadata": {
        "id": "xEy4mu8iJSGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "hub-Bva883_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization and transform into Ascii"
      ],
      "metadata": {
        "id": "zVNE6bduUMJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ],
      "metadata": {
        "id": "yn9ZOZ8l9v6c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "# Test the unicodeToAscii function\n",
        "input_strings = [\n",
        "    \"Caf√©\",\n",
        "    \"r√©sum√©\",\n",
        "    \"r√©serv√©\",\n",
        "    \"jalape√±o\",\n",
        "    \"√ºber\",\n",
        "]\n",
        "\n",
        "for input_string in input_strings:\n",
        "    ascii_string = unicodeToAscii(input_string)\n",
        "    print(\"Original:\", input_string)\n",
        "    print(\"ASCII:\", ascii_string)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YmsuvQ_TolZ",
        "outputId": "52637fec-6c7a-48c1-a88a-d5ae79c5ceb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Caf√©\n",
            "ASCII: Cafe\n",
            "\n",
            "Original: r√©sum√©\n",
            "ASCII: resume\n",
            "\n",
            "Original: r√©serv√©\n",
            "ASCII: reserve\n",
            "\n",
            "Original: jalape√±o\n",
            "ASCII: jalapeno\n",
            "\n",
            "Original: √ºber\n",
            "ASCII: uber\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the normalizeString function\n",
        "input_strings = [\n",
        "    \"Hello, world!\",\n",
        "    \"This is a test string.\",\n",
        "    \"I love OpenAI's models!\",\n",
        "    \"It's a beautiful day üåû\",\n",
        "]\n",
        "\n",
        "for input_string in input_strings:\n",
        "    normalized_string = normalizeString(input_string)\n",
        "    print(\"Original:\", input_string)\n",
        "    print(\"Normalized:\", normalized_string)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2FSm6IjT0V7",
        "outputId": "cefa58af-d6af-4c34-9cc3-f7f6e0a7eb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Hello, world!\n",
            "Normalized: hello world !\n",
            "\n",
            "Original: This is a test string.\n",
            "Normalized: this is a test string\n",
            "\n",
            "Original: I love OpenAI's models!\n",
            "Normalized: i love openai s models !\n",
            "\n",
            "Original: It's a beautiful day üåû\n",
            "Normalized: it s a beautiful day\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Language"
      ],
      "metadata": {
        "id": "t1SDvyt9UbSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # ƒê·ªçc n·ªôi dung t·ª´ t·ªáp ng√¥n ng·ªØ ngu·ªìn (English)\n",
        "    with open('train.en', encoding='utf-8') as f:\n",
        "        lines_en = f.read().strip().split('\\n')\n",
        "\n",
        "    # ƒê·ªçc n·ªôi dung t·ª´ t·ªáp ng√¥n ng·ªØ ƒë√≠ch (Vietnamese)\n",
        "    with open('train.vi', encoding='utf-8') as f:\n",
        "        lines_vi = f.read().strip().split('\\n')\n",
        "\n",
        "    # Ki·ªÉm tra s·ªë d√≤ng c√≥ b·∫±ng nhau kh√¥ng\n",
        "    if len(lines_en) != len(lines_vi):\n",
        "        raise ValueError(\"Number of lines in train.en and train.vi do not match.\")\n",
        "\n",
        "    pairs = [[normalizeString(en), vi] for en, vi in zip(lines_en, lines_vi)]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "8MBDNlVw93vf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 128\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "nfDDrDesi639"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = readLangs(\"eng\", \"viet\")\n",
        "\n",
        "# Print some information about the data\n",
        "print(\"Input language:\", input_lang.name)\n",
        "print(\"Output language:\", output_lang.name)\n",
        "print(\"Number of sentence pairs:\", len(pairs))\n",
        "\n",
        "# Print some example pairs\n",
        "print(\"\\nExample pairs:\")\n",
        "for i in range(5):\n",
        "    print(\"Input:\", pairs[i][0])\n",
        "    print(\"Output:\", pairs[i][1])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UZ57tj5UeIJ",
        "outputId": "584a8043-0b56-42d5-a486-617fd42450b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Input language: eng\n",
            "Output language: viet\n",
            "Number of sentence pairs: 133317\n",
            "\n",
            "Example pairs:\n",
            "Input: rachel pike the science behind a climate headline\n",
            "Output: Khoa h·ªçc ƒë·∫±ng sau m·ªôt ti√™u ƒë·ªÅ v·ªÅ kh√≠ h·∫≠u\n",
            "\n",
            "Input: in minutes atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change with her team one of thousands who contributed taking a risky flight over the rainforest in pursuit of data on a key molecule\n",
            "Output: Trong 4 ph√∫t , chuy√™n gia ho√° h·ªçc kh√≠ quy·ªÉn Rachel Pike gi·ªõi thi·ªáu s∆° l∆∞·ª£c v·ªÅ nh·ªØng n·ªó l·ª±c khoa h·ªçc mi·ªát m√†i ƒë·∫±ng sau nh·ªØng ti√™u ƒë·ªÅ t√°o b·∫°o v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , c√πng v·ªõi ƒëo√†n nghi√™n c·ª©u c·ªßa m√¨nh -- h√†ng ng√†n ng∆∞·ªùi ƒë√£ c·ªëng hi·∫øn cho d·ª± √°n n√†y -- m·ªôt chuy·∫øn bay m·∫°o hi·ªÉm qua r·ª´ng gi√† ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin v·ªÅ m·ªôt ph√¢n t·ª≠ then ch·ªët .\n",
            "\n",
            "Input: i apos d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper\n",
            "Output: T√¥i mu·ªën cho c√°c b·∫°n bi·∫øt v·ªÅ s·ª± to l·ªõn c·ªßa nh·ªØng n·ªó l·ª±c khoa h·ªçc ƒë√£ g√≥p ph·∫ßn l√†m n√™n c√°c d√≤ng t√≠t b·∫°n th∆∞·ªùng th·∫•y tr√™n b√°o .\n",
            "\n",
            "Input: headlines that look like this when they have to do with climate change and headlines that look like this when they have to do with air quality or smog\n",
            "Output: C√≥ nh·ªØng d√≤ng tr√¥ng nh∆∞ th·∫ø n√†y khi b√†n v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , v√† nh∆∞ th·∫ø n√†y khi n√≥i v·ªÅ ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ hay kh√≥i b·ª•i .\n",
            "\n",
            "Input: they are both two branches of the same field of atmospheric science\n",
            "Output: C·∫£ hai ƒë·ªÅu l√† m·ªôt nh√°nh c·ªßa c√πng m·ªôt lƒ©nh v·ª±c trong ng√†nh khoa h·ªçc kh√≠ quy·ªÉn .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'viet', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BcnCa2QSfmn",
        "outputId": "5340ec82-f5d9-4fbd-cb65-0df5b353ee4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 133317 sentence pairs\n",
            "Trimmed to 133095 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "viet 25435\n",
            "eng 40979\n",
            "['V√† khi ti·∫øn h√†nh n√≥ , c√≥ nh·ªØng √Ω nghƒ© kh√°c xu·∫•t hi·ªán l·∫°i trong ƒë·∫ßu c√¥ .', 'and as she did other ones came to mind']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "_FibivZKQvdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return output, (hidden, cell)"
      ],
      "metadata": {
        "id": "5YkO-Mb0ShEZ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden, decoder_cell = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, decoder_cell = self.forward_step(decoder_input, decoder_hidden, decoder_cell)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, (decoder_hidden, decoder_cell), None\n",
        "\n",
        "    def forward_step(self, input, hidden, cell):\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = F.relu(embedded)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        output = self.out(output)\n",
        "        return output, hidden, cell"
      ],
      "metadata": {
        "id": "WLOx43d_Qzo7"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionModel, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = AttentionModel(hidden_size)\n",
        "        self.lstm = nn.LSTM(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden, decoder_cell = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, (decoder_hidden, decoder_cell), attn_weights = self.forward_step(\n",
        "                decoder_input, (decoder_hidden, decoder_cell), encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, (decoder_hidden, decoder_cell), attentions\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden[0].permute(1, 0, 2)  # hidden[0] is the hidden state in LSTM\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_lstm = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(input_lstm, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, (hidden, cell), attn_weights"
      ],
      "metadata": {
        "id": "bZxi7-4_SaNH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-aWcdJ8njdab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'viet', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "IzzZD8b-jeyF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "AtrfcZIajgs6"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "ZeJ-UGaIjjvj"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.switch_backend('agg')\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "0BoF2hpAjwsG"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "OCLkMtORjnqD"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "8QFfO89CjoJD"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "9mSSmRzvkAg0"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttentionDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "id": "l1L6kMfekCpE",
        "outputId": "82405186-f04c-43f0-8b84-6072a9ac04ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 133317 sentence pairs\n",
            "Trimmed to 133095 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "viet 25435\n",
            "eng 40979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "id": "0LBK0x6ukMvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2HvLnA_HNEN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}